# unilm
**unified pre-training for language understanding (nlu) and generation (nlg)**

语言理解(NLU)和生成(NLG)的统一预训练

update (june, 2020): [unilmv2: pseudo-masked language models for unified language model pre-training](https://arxiv.org/abs/2002.12804) was accepted by icml 2020.

**unilm v2** ```new``` (february, 2020): "[unilmv2: pseudo-masked language models for unified language model pre-training](https://arxiv.org/abs/2002.12804)".

**[unilm v1](https://github.com/microsoft/unilm/tree/master/unilm-v1)** (september 30th, 2019): the code and pre-trained models for the neurips 2019 paper entitled "[unified language model pre-training for natural language understanding and generation](https://arxiv.org/abs/1905.03197)".

## citation

if you find unilm useful in your work, you can cite the following paper:
```
@inproceedings{unilmv2,
    title={UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training},
    author={Bao, Hangbo and Dong, Li and Wei, Furu and Wang, Wenhui and Yang, Nan and Liu, Xiaodong and Wang, Yu and Piao, Songhao and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
    year={2020},
    booktitle = "Preprint"
}
```

## acknowledgments
our code is based on [pytorch-transformers v0.4.0](https://github.com/huggingface/pytorch-transformers/tree/v0.4.0). we thank the authors for their wonderful open-source efforts.

## license
this project is licensed under the license found in the license file in the root directory of this source tree.
portions of the source code are based on the [pytorch-transformers v0.4.0](https://github.com/huggingface/pytorch-transformers/tree/v0.4.0) project.

[microsoft open source code of conduct](https://opensource.microsoft.com/codeofconduct)

### contact Information

for help or issues using unilm, please submit a github issue.

for other communications related to unilm, please contact li dong (`lidong1@microsoft.com`), furu wei (`fuwei@microsoft.com`).
